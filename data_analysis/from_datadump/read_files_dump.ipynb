{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d927df3-fd29-4bd2-9e95-bdd28abdd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import fasttext \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3491d-29ba-4994-bfca-d993127d072d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis on data dump\n",
    "This notebook is to learn reading data dumps directly form the server (Rnd2) as opposed to obtain the data through API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308626d-760c-41ea-8f70-fb2c8fe91237",
   "metadata": {},
   "source": [
    "Recent data dump with normalized lang tags produced at the end of June 2022 for the project Etranslate is used in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04302a5c-f959-4f7e-97ea-2c6361d8ce69",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ad45a8-7c4f-46a6-b842-afb798de35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data dump Europeana June 2022 location on the Rnd2 server\n",
    "data_path=\"/projects/etranslate-data-dump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b170d1-fcdd-4164-ac6b-2ba6449b088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pscalia/git/rd-europeana-translate/data_analysis/from_datadump\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b397922b-9447-4146-8a00-475e81b00c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing files in directory\n",
    "list_data_files=os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae781532-ddd4-49ff-a947-b752d525dbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2089"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf8b4e-27bf-416a-bcc7-a39178bbe626",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Experiment with zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20f02373-dd78-4bb7-bd57-a8f3aad6db8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/etranslate-data-dump/02301.zip'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#locating zip file. Each zip file  contains a collection\n",
    "file_name = os.path.join(data_path,'02301.zip')\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "193921f2-4244-4dfe-a6a1-f4a60d74fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(file_name, 'r') as zipObject:\n",
    "     listOfFileNames = zipObject.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04f3b1-51be-49cf-82e6-c424d0e12ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all files that have not been extracted within a given zip file\n",
    "with ZipFile(file_name, 'r') as zipObject:\n",
    " listOfFileNames = zipObject.namelist()\n",
    " for fileName in listOfFileNames:\n",
    "        if os.path.isfile(fileName):\n",
    "            print('file already exist')\n",
    "        else:\n",
    "            zipObject.extract(fileName)\n",
    "            print('file extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5578467-66a0-4ab9-a48b-7fbcb8c287ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'urn_imss_biography_020216.xml'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfFileNames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c51c183-2505-4212-83ca-fda826e67064",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat='00180020C7AF376F0C82A5F47CAD7BED272DF62A.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23b0ae9a-25c9-48eb-914e-2029e0b6b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open one of the xml unzipped files\n",
    "with open(listOfFileNames[0], 'r') as f:\n",
    "\tfile = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f26e605-4fbc-4e84-890c-15eb53ca4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use beaut soup for parsing xml\n",
    "soup = BeautifulSoup(file, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b281d2b-d47c-4233-887d-83a2adc67484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding all description tag\n",
    "t=soup.find('description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad754c10-e135-43f2-866e-4a1a38281328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"After a humanist education in fifteenth-century Florence, Amerigo Vespucci engaged in commercial and financial ventures. Sent to Seville by Lorenzo di Pierfrancesco de' Medici, he settled there and began to work with the Spanish and Portuguese travelers who, in the wake of Christopher Columbus, [...]\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c62f56e-cf18-4363-b3ec-090454dd161e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get('xml:lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28651b98-b4bd-438d-8183-6433389e39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find attributes of title\n",
    "soup.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07948a09-d255-4083-abdc-b5a1c22e885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame([t.text], columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017df05-d865-4530-b3ce-9538cdbe23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633636b-e238-47be-b6fb-1afedaa3d0a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Read all files from a given collection and build clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6590e-823e-4c1b-ad84-0186f575e888",
   "metadata": {},
   "source": [
    "This is built upon https://www.jeansnyman.com/posts/unsupervised-text-clustering-with-k-means/ The idea is to look at ways to measure similarities among texts. The goal is to estimate similar texts to Europeana in order to find texts in low represented languages that can be used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b42762a4-f495-4ee7-b38f-870a6de1e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some text cleanup\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "REMOVE_NUM = re.compile('[\\d+]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    text: a string\n",
    "    return: modified initial string\n",
    "    \"\"\"\n",
    "    # lowercase text\n",
    "    text = text.lower() \n",
    "\n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "    \n",
    "    # Remove the XXXX values\n",
    "    text = text.replace('x', '') \n",
    "    \n",
    "    # Remove white space\n",
    "    text = REMOVE_NUM.sub('', text)\n",
    "\n",
    "    #  delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) \n",
    "\n",
    "    # delete stopwords from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
    "    \n",
    "    # removes any words composed of less than 2 or more than 21 letters\n",
    "    text = ' '.join(word for word in text.split() if (len(word) >= 2 and len(word) <= 21))\n",
    "\n",
    "    # Stemming the words\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c15245-30f8-4998-bf65-4bcebe15f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_collection(listOfFileNames, field):\n",
    "    descriptions=[]\n",
    "    for filename in listOfFileNames:\n",
    "        with open(filename, 'r') as f:\n",
    "            file = f.read() \n",
    "        soup = BeautifulSoup(file, 'xml')\n",
    "        t=soup.find(f'{field}')\n",
    "        if t:\n",
    "            if t.get('xml:lang')=='en':\n",
    "                descriptions.append(t.text)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7a16d53-189c-4e56-bb84-bebd966ecc7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'listOfFileNames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20462/2061916223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_single_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfFileNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'description'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'listOfFileNames' is not defined"
     ]
    }
   ],
   "source": [
    "des=read_single_collection(listOfFileNames, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54d214b-a1f0-4a9f-b06d-30371b6c3214",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'des' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20462/683994221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'des' is not defined"
     ]
    }
   ],
   "source": [
    "des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ddf80-c701-4444-b2a2-8ca4171d1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(des, columns=[\"descriptions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f09e4c87-e9c6-43e3-b46a-df3f5c15d6ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20462/2054361833.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5aa1a2-8bae-4e90-94f8-22f5dfddee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"descriptions\"] =df[\"descriptions\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785142f1-0453-4f99-8d52-31d7fa6fb55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1, 2), stop_words='english')\n",
    "X_train_vc = vectorizer.fit_transform(df[\"descriptions\"])\n",
    "\n",
    "pd.DataFrame(X_train_vc.toarray(), columns=vectorizer.get_feature_names_out()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b90dff-b88a-4045-a9d7-8d3da8594120",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_clusters = 1\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d34be-e36e-4c5b-9e2a-23a2357a6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "score = []\n",
    "for i in range(1,k_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=800,n_init=5,random_state=0)\n",
    "    kmeans.fit(X_train_vc)\n",
    "    score.append(kmeans.inertia_)\n",
    "plt.plot(range(1,k_clusters + 1 ),score)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.savefig('elbow.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d545d-e481-4bf8-a122-86e134a7746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_clusters = 1\n",
    "\n",
    "model = KMeans(n_clusters=k_clusters, init='k-means++', n_init=10, max_iter=1000, tol=0.001, random_state=0)\n",
    "model.fit(X_train_vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238f734-80e7-4d7c-a50f-6d84061c10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = model.predict(X_train_vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d8997-b444-4c6c-b947-87983d98f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ClusterName\"] = clusters\n",
    "# Convert the label (Product) to numeric using the pd factorize function \n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c0564-29e4-439a-993d-60bb2de35572",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y_kmeans==0, 0], X[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa7b32-18f5-4eab-ab5f-5b15be7f44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "sklearn_pca = PCA(n_components = 2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_train_vc.toarray())\n",
    "kmeans = KMeans(n_clusters=k_clusters, max_iter=600, algorithm = 'lloyd')\n",
    "fitted = kmeans.fit(Y_sklearn)\n",
    "prediction = kmeans.predict(Y_sklearn)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=prediction, s=40, cmap='viridis', linewidths=5)\n",
    "\n",
    "centers = fitted.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1],c='black', s=200, alpha=0.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337a402-cd86-4e56-92a9-e85c1cafcb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fac970-10cd-4304-a2f5-4af89157c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
    "    labels = np.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # indices for each cluster\n",
    "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        features = vectorizer.get_feature_names_out()\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def plotWords(dfs, n_feats):\n",
    "    for i in range(0, len(dfs)):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096e92b-d89a-4ab7-ba31-4d7730de71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = get_top_features_cluster(X_train_vc.toarray(), prediction, 20)\n",
    "plotWords(dfs, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d17dc-8a2b-4d9c-afc2-ddf3222806a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [\"the instrument has a scale, dial brass and it is wooden\",\n",
    "                \"use \",\n",
    "                \" \"]\n",
    "cleaned_data = pd.DataFrame(cleaned_data, columns=[\"descriptions\"])\n",
    "cleaned_data = cleaned_data[\"descriptions\"].apply(clean_text)\n",
    "predicted = model.predict(vectorizer.transform(cleaned_data))\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf74dd-0d26-4210-86ab-cdf2514bb25e",
   "metadata": {},
   "source": [
    "The problem with is approach is that it does not quantify how well the sentence in the example matches its belonging to the cluster. It is probably related to the fact that it is treated as a unsupervised problem.\n",
    "Idea to explore- use clustering in a supervised way using the name of the collections al cluster name! Would that work? Maybe even better would be using fasttext which is designed for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53acce-f837-4dd7-b0fc-62491c4279d2",
   "metadata": {},
   "source": [
    "# Experiment using fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278a017-f57e-4e12-b055-aacb589d4d6b",
   "metadata": {},
   "source": [
    "The idea is to use the fasttext classifier to classify all descriptions into labels. In this experiment we use the names of the datasets as label names. This is a supervised training alogorithm. If this works well we could then use new sentences from different datasets and measure their classification score into one of the labels. If the classification score is high enough we could add the sentence to the training datasets. <br>\n",
    "Reference https://towardsdatascience.com/fasttext-for-text-classification-a4b38cbff27c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd105b46-2f39-4396-b651-74e9cd0ea4d7",
   "metadata": {},
   "source": [
    "First we have to organize the descriptions in the classes given by the datasetname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0cb6a8f-582a-4180-a3b6-7f14c989dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_collection_(listOfFileNames, field,zip_):\n",
    "    \"\"\" This function organizes metadata fields from a collection in a daframe\n",
    "     Parameters\n",
    "     listOfFileNames: list of file names with the given collection\n",
    "     field: metadata field value to extract (only english values), typically title or description\n",
    "     zip: reference Zipobject to be read\n",
    "    (eg. a collection could be of the type 3204.zip) \"\"\"\n",
    "    descriptions=[]\n",
    "    for filename in listOfFileNames:\n",
    "        if os.path.isfile(filename)!= None:\n",
    "            zip_.extract(filename)\n",
    "            with open(filename, 'r') as f:\n",
    "                file = f.read() \n",
    "                soup = BeautifulSoup(file, 'xml')\n",
    "                t=soup.find(f'{field}')\n",
    "                if t:\n",
    "                    if t.get('xml:lang')=='en': #extracting field with english language tag\n",
    "                        descriptions.append(t.text)\n",
    "        os.remove(filename)  \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d28689b-7c78-47f7-9cff-aa715e139aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02301.zip', '2058501.zip', '03706.zip', '07101.zip']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of datasets to be considered\n",
    "list_datasets_analysis=['02301.zip','2058501.zip']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1d91f-3ab7-4926-b780-f3735c070cfd",
   "metadata": {},
   "source": [
    "Now we read the files from the collections, and categorize them according to their collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d275d4-839c-4e44-aa20-f5320df776eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This for loop creates a df for each collection where the first column represents the value of a metadatafield\n",
    "# and the second column assigns it as label the name of the collection\n",
    "list_of_dfs = []\n",
    "for dataset in list_data_files:\n",
    "    file_name = os.path.join(data_path, dataset)\n",
    "    with ZipFile(file_name, 'r') as zipObject:\n",
    "        listOfFileNames = zipObject.namelist()\n",
    "        des=read_single_collection_(listOfFileNames, 'description',zipObject)\n",
    "        df=pd.DataFrame(des, columns=[\"descriptions\"])\n",
    "        df['label']=f'__label__{dataset.split(\".\")[0]}'\n",
    "        list_of_dfs.append(df)\n",
    "list_of_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86c58824-360f-4c48-8377-acc34f85b75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descriptions</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After a humanist education in fifteenth-centur...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lavoisier began his legal studies at the insti...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>French chemist and physicist. After attending ...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A pharmacist in Rouen and student of G.-F. Rou...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The last child of a family involved in pottery...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        descriptions           label\n",
       "0  After a humanist education in fifteenth-centur...  __label__02301\n",
       "1  Lavoisier began his legal studies at the insti...  __label__02301\n",
       "2  French chemist and physicist. After attending ...  __label__02301\n",
       "3  A pharmacist in Rouen and student of G.-F. Rou...  __label__02301\n",
       "4  The last child of a family involved in pottery...  __label__02301"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a single dataframe with all collections\n",
    "df_fin=pd.concat(list_of_dfs, ignore_index=True, axis=0)\n",
    "df_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3b016f9-933c-4733-b640-429c0e29fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using jensin to do NLP Preprocessing and prepare the data for fasttext\n",
    "df_fin.iloc[:, 0] = df_fin.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45a2b319-5215-4608-8705-4f20978c2c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descriptions</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>after humanist education in fifteenth century ...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lavoisier began his legal studies at the insti...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>french chemist and physicist after attending a...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pharmacist in rouen and student of rouelle des...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the last child of family involved in pottery s...</td>\n",
       "      <td>__label__02301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        descriptions           label\n",
       "0  after humanist education in fifteenth century ...  __label__02301\n",
       "1  lavoisier began his legal studies at the insti...  __label__02301\n",
       "2  french chemist and physicist after attending a...  __label__02301\n",
       "3  pharmacist in rouen and student of rouelle des...  __label__02301\n",
       "4  the last child of family involved in pottery s...  __label__02301"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092c914-2236-4b49-aa96-4bd13f3274e3",
   "metadata": {},
   "source": [
    "Splitting the dataframe randomly into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7daaee58-ed19-4bbd-af11-c272733b0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_fin, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc3aad38-2213-4afd-a54c-4be7c345429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveing the train data to txt file\n",
    "train.to_csv('train.txt', index=False, sep=' ', header=False, escapechar=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b81420a-8fcc-497c-b472-fb9b5a67c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveing the test data to txt file\n",
    "test.to_csv('test.txt', index=False, sep=' ', header=False, escapechar=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8528aac9-543b-4c07-b4f9-04f3a0f18ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  7247\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  363255 lr:  0.000000 avg.loss:  0.050277 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "model = fasttext.train_supervised('train.txt', wordNgrams = 2, epoch=5,lr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "022afe36-20a2-4ebf-87db-ff5fbff24349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "descriptions    dagger shaped compass for geometrical surveyin...\n",
       "label                                              __label__02301\n",
       "Name: 547, dtype: object"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4aab6436-53a6-4a87-ac38-8913e4fec76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "descriptions    mechanical device for the transmission and reg...\n",
       "label                                              __label__02301\n",
       "Name: 177, dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[18,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1021381d-c425-4c25-ba00-87c7294714d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__02301',), array([0.99950182]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test.iloc[18,0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "650b99e0-c476-49ad-96a2-063d259a7610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 0.990228013029316, 0.990228013029316)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('test.txt')                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba81f4-2579-4ba4-9f76-c5efa8fe1ff2",
   "metadata": {},
   "source": [
    "Now I test the category prediction of a random sentence taken not from Europeana and use the trained model\n",
    "to assign it a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "139614a7-dc8d-405c-83da-efd1c6b5f5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__2058501',), array([1.00000989]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text copied by the new yorker\n",
    "model.predict('what do you think')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdbd5c-89c5-4e0f-9925-d58e7ec45196",
   "metadata": {},
   "source": [
    "Here I know that the sentence does not belong to Europeana but still the model assign it to an Europeana collection with high confidence - aboyt 86% - it probably means that at this stage we cannot use this system \n",
    "to judge if data is in domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b0f7d-d0a2-4308-b436-e18e60f8d859",
   "metadata": {},
   "source": [
    "# Experiment: extract all german text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7eee5-5531-4056-a4e6-cfc6d9075915",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cs.calculate(df.iloc[0], df.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc642c66-a059-46d5-93be-17feee156589",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfFileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d41b6-ca12-4971-bd52-9059aa711bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(file_name, 'r') as zipObject:\n",
    " listOfFileNames = zipObject.namelist()\n",
    " for fileName in listOfFileNames:\n",
    "        if os.path.isfile(fileName):\n",
    "           # Extract a single file from zip\n",
    "            print('file already exist')\n",
    "            print(fileName)\n",
    "        else:\n",
    "            zipObject.extract(fileName)\n",
    "            print('file extracted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f5305-e502-48fe-a4f2-35113255c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_collection(listOfFileNames, field):\n",
    "    lang_tags=[]\n",
    "    for filename in listOfFileNames:\n",
    "        with open(filename, 'r') as f:\n",
    "            file = f.read() \n",
    "        soup = BeautifulSoup(file, 'xml')\n",
    "        t=soup.find(f'{field}')\n",
    "        lang=t.get('xml:lang')\n",
    "        if lang:\n",
    "            lang_tags.append(lang)\n",
    "    return lang_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8b479-2c19-4b89-8992-9a3001a50a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_files=os.listdir(data_path) #list of zip files\n",
    "for zip_file in list_data_files:\n",
    "    file_name = os.path.join(data_path,  zip_file) #name of zip file\n",
    "    print(file_name)\n",
    "    with ZipFile(file_name, 'r') as zipObject:\n",
    "        listOfFileNames = zipObject.namelist()\n",
    "        for fileName in listOfFileNames:\n",
    "                if os.path.isfile(fileName):\n",
    "                   # Extract a single file from zip\n",
    "                   print('file exists')\n",
    "                   os.remove(fileName)  \n",
    "                else:\n",
    "                    pass\n",
    "         \n",
    "                #     extracted_file=zipObject.extract(fileName)\n",
    "                #     print('file extracted')\n",
    "                #     with open( extracted_file, 'r') as f:\n",
    "                #         file = f.read() \n",
    "                #     soup = BeautifulSoup(file, 'xml')\n",
    "                #     t=soup.find('description')\n",
    "                #     print(t.get('xml:lang'))\n",
    "                #     print('file extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5810d-b794-432f-8347-588ad65e185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for values in soup.findAll(\"time\"):\n",
    "    print(\"{} : {}, {}Â°\".format(values[\"from\"], values.find(\"symbol\")[\"name\"], values.fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc314ef-c742-4c25-a22f-9e016621c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path,  '2051943.zip')\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b3a50-4523-42b9-990f-bf4c554c4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plink__f_1_100475.xml', 'r') as f:\n",
    "    file = f.read() \n",
    "soup = BeautifulSoup(file, 'xml')\n",
    "t=soup.find('temporal')\n",
    "#t.get('xml:lang')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb65dab-6eed-4b44-8350-f924c820f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path,  '2051943.zip')\n",
    "file_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95779a2c-1039-4988-bb13-51a6d27c2830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8ec4b-b282-40be-85d4-baa7ae072eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path,  list_data_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea414d-bb2d-4afd-a33b-b27e01b920d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(file_name, 'r') as zipObject:\n",
    "    listOfFileNames = zipObject.namelist()\n",
    " for fileName in listOfFileNames:\n",
    "        if os.path.isfile(fileName):\n",
    "           # Extract a single file from zip\n",
    "            print('file already exist')\n",
    "        else:\n",
    "            data=zipObject.extract(fileName)\n",
    "            print('file extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87220c18-4c10-447c-9fdd-01f09dffe741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative to find keyword in file?\n",
    "keyword = 'your_keyword'\n",
    "for file in files:\n",
    "    if os.path.isfile(os.path.join(your_path, file)):\n",
    "        f = open(os.path.join(your_path, file),'r')\n",
    "        for x in f:\n",
    "            if keyword in x:\n",
    "                #do what you want\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
